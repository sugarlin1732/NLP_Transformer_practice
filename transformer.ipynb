{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsWcFbPGUbXbv9LLniXQGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarlin1732/NLP_Transformer_practice/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B27_o5bWhTq5",
        "colab_type": "code",
        "outputId": "5cc971df-f846-4c02-a05c-016973ce7136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# import lib\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "from IPython.display import clear_output\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qyYDWmkIKIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \".//gdrive//My Drive//Transformer_practice//\"\n",
        "if not os.path.isdir(path):\n",
        "    os.mkdir(path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfxYox4g7vh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ignore unnecessary warnings\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "np.set_printoptions(suppress=True) # disable scientific notation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7I0GHWM8fZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download training data (\"wmt19_translate\")\n",
        "# make download dir\n",
        "download_dir = \".//download_dir\"\n",
        "if not os.path.isdir(download_dir):\n",
        "    os.mkdir(download_dir)\n",
        "\n",
        "config = tfds.translate.wmt.WmtConfig(\n",
        "    version=\"0.0.1\",\n",
        "    language_pair=(\"zh\", \"en\"), # zh-en translation\n",
        "    subsets={\n",
        "        tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
        "    },\n",
        ")\n",
        "builder = tfds.builder(\"wmt_translate\", config=config)\n",
        "\n",
        "builder.download_and_prepare(download_dir=download_dir)\n",
        "clear_output()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA9jv4XzLAVR",
        "colab_type": "code",
        "outputId": "58d397de-34f3-4cf3-8ebe-d7d1b36314bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# dataset split\n",
        "\n",
        "training_set, val_set, testing_set = builder.as_dataset(split=\\\n",
        "    (\"train[:50%]\", \"train[50%:75%]\", \"train[75%:]\") \\\n",
        "    , as_supervised=True)\n",
        "print(training_set)\n",
        "print(val_set)\n",
        "print(testing_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>\n",
            "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>\n",
            "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sejKMBwdtRyc",
        "colab_type": "code",
        "outputId": "06ea322a-9ba4-4dd5-86f9-50f9463bb9db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "# check data\n",
        "for en, zh in training_set.take(2):\n",
        "  print(en)\n",
        "  print(zh)\n",
        "  print('-' * 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'The fear is real and visceral, and politicians ignore it at their peril.', shape=(), dtype=string)\n",
            "tf.Tensor(b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82', shape=(), dtype=string)\n",
            "----------\n",
            "tf.Tensor(b'In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word \\xe2\\x80\\x9cliberal\\xe2\\x80\\x9d \\xe2\\x80\\x93 a champion of the cause of individual freedom.', shape=(), dtype=string)\n",
            "tf.Tensor(b'\\xe4\\xba\\x8b\\xe5\\xae\\x9e\\xe4\\xb8\\x8a\\xef\\xbc\\x8c\\xe5\\xbe\\xb7\\xe5\\x9b\\xbd\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xb1\\x80\\xe5\\x8a\\xbf\\xe9\\x9c\\x80\\xe8\\xa6\\x81\\xe7\\x9a\\x84\\xe4\\xb8\\x8d\\xe8\\xbf\\x87\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe7\\xac\\xa6\\xe5\\x90\\x88\\xe7\\xbe\\x8e\\xe5\\x9b\\xbd\\xe6\\x89\\x80\\xe8\\xb0\\x93\\xe2\\x80\\x9c\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe2\\x80\\x9d\\xe5\\xae\\x9a\\xe4\\xb9\\x89\\xe7\\x9a\\x84\\xe7\\x9c\\x9f\\xe6\\xad\\xa3\\xe7\\x9a\\x84\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe5\\x85\\x9a\\xe6\\xb4\\xbe\\xef\\xbc\\x8c\\xe4\\xb9\\x9f\\xe5\\xb0\\xb1\\xe6\\x98\\xaf\\xe4\\xb8\\xaa\\xe4\\xba\\xba\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe4\\xba\\x8b\\xe4\\xb8\\x9a\\xe7\\x9a\\x84\\xe5\\x80\\xa1\\xe5\\xaf\\xbc\\xe8\\x80\\x85\\xe3\\x80\\x82', shape=(), dtype=string)\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3mnSErbt4mJ",
        "colab_type": "code",
        "outputId": "59ae5c70-c6a0-4c37-8176-9080a0fd755f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "# numpy decode the coding of examples and save as samples\n",
        "samples = []\n",
        "sample_num = 10\n",
        "for en_raw, zh_raw in training_set.take(sample_num):\n",
        "    en = en_raw.numpy().decode(\"utf-8\")\n",
        "    zh = zh_raw.numpy().decode(\"utf-8\")\n",
        "    print(en)\n",
        "    print(zh)\n",
        "    samples.append((en, zh))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The fear is real and visceral, and politicians ignore it at their peril.\n",
            "这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n",
            "In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word “liberal” – a champion of the cause of individual freedom.\n",
            "事实上，德国政治局势需要的不过是一个符合美国所谓“自由”定义的真正的自由党派，也就是个人自由事业的倡导者。\n",
            "Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.\n",
            "必须付出巨大的努力和基础设施投资才能完成向可再生能源的过渡。\n",
            "In this sense, it is critical to recognize the fundamental difference between “urban villages” and their rural counterparts.\n",
            "在这方面，关键在于认识到“城市村落”和农村村落之间的根本区别。\n",
            "A strong European voice, such as Nicolas Sarkozy’s during the French presidency of the EU, may make a difference, but only for six months, and at the cost of reinforcing other European countries’ nationalist feelings in reaction to the expression of “Gallic pride.”\n",
            "法国担任轮值主席国期间尼古拉·萨科奇统一的欧洲声音可能让人耳目一新，但这种声音却只持续了短短六个月，而且付出了让其他欧洲国家在面对“高卢人的骄傲”时民族主义情感进一步被激发的代价。\n",
            "Most of Japan’s bondholders are nationals (if not the central bank) and have an interest in political stability.\n",
            "日本债券持有人大多为本国国民（甚至中央银行 ） ， 政治稳定符合他们的利益。\n",
            "Paul Romer, one of the originators of new growth theory, has accused some leading names, including the Nobel laureate Robert Lucas, of what he calls “mathiness” – using math to obfuscate rather than clarify.\n",
            "新增长理论创始人之一的保罗·罗默（Paul Romer）也批评一些著名经济学家，包括诺贝尔奖获得者罗伯特·卢卡斯（Robert Lucas）在内，说他们“数学性 ” （ 罗默的用语）太重，结果是让问题变得更加模糊而不是更加清晰。\n",
            "It is, in fact, a capsule depiction of the United States Federal Reserve and the European Central Bank.\n",
            "事实上，这就是对美联储和欧洲央行的简略描述。\n",
            "Given these variables, the degree to which migration is affected by asylum-seekers will not be easy to predict or control.\n",
            "考虑到这些变量，移民受寻求庇护者的影响程度很难预测或控制。\n",
            "WASHINGTON, DC – In the 2016 American presidential election, Hillary Clinton and Donald Trump agreed that the US economy is suffering from dilapidated infrastructure, and both called for greater investment in renovating and upgrading the country’s public capital stock.\n",
            "华盛顿—在2016年美国总统选举中，希拉里·克林顿和唐纳德·特朗普都认为美国经济饱受基础设施陈旧的拖累，两人都要求加大投资用于修缮和升级美国公共资本存量。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa5iirn4xzUg",
        "colab_type": "code",
        "outputId": "05849956-9217-4364-9032-c1965d717dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# make dictionary\n",
        "# use SubwordTextEncoder to convert word into numbers\n",
        "%%time\n",
        "# make en dictionary\n",
        "try:\n",
        "    # load existed dictionary if exists\n",
        "    subword_en = tfds.features.text.SubwordTextEncoder.load_from_file( \\\n",
        "                path + \"en_dictionary\")\n",
        "except:\n",
        "    subword_en = tfds.features.text.SubwordTextEncoder.build_from_corpus( \\\n",
        "                (en_word.numpy() for en_word, zh_word in training_set), \\\n",
        "                target_vocab_size = 2**15)\n",
        "                # en_word.numpy() no need to decode\n",
        "    subword_en.save_to_file(path + \"en_dictionary\") # save dictionary\n",
        "print(f\"英文字典大小：{subword_en.vocab_size}\")\n",
        "print(f\"前 10 個英文  subwords：{subword_en.subwords[:10]}\")\n",
        "\n",
        "# make zh dictionary\n",
        "try:\n",
        "    subword_zh = tfds.features.text.SubwordTextEncoder.load_from_file( \\\n",
        "                path + \"zh_dictionary\")\n",
        "except:\n",
        "    subword_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus( \\\n",
        "                (zh_word.numpy() for en_word, zh_word in training_set), \\\n",
        "                target_vocab_size = 2**15)\n",
        "                # zh_word.numpy() no need to decode\n",
        "    subword_zh.save_to_file(path + \"zh_dictionary\") # save dictionary\n",
        "\n",
        "\n",
        "print(f\"中文字典大小：{subword_zh.vocab_size}\")\n",
        "print(f\"前 10 個中文  subwords：{subword_zh.subwords[:10]}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "英文字典大小：32007\n",
            "前 10 個英文  subwords：[', ', 'the_', 'to_', 'of_', 'and_', 'in_', 'a_', 'that_', 'is_', 's_']\n",
            "中文字典大小：32061\n",
            "前 10 個中文  subwords：['，', '。', '、', ' — — ', '（', '“', '的', '）', '”', '和']\n",
            "CPU times: user 295 ms, sys: 7.15 ms, total: 302 ms\n",
            "Wall time: 721 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uvpQZhlezHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add BOS and EOS into sentence\n",
        "# 加入起始以及結束token，編碼定義為字典最後加兩碼(subword_en.vocab_size, subword_en.vocab_size+1)\n",
        "\n",
        "def add_BEOS(en_sentence, zh_sentence):\n",
        "    en_tmp = [subword_en.vocab_size] + subword_zh.encode(en_sentence.numpy()) \\\n",
        "    + [subword_en.vocab_size+1]\n",
        "    zh_tmp = [subword_zh.vocab_size] + subword_zh.encode(zh_sentence.numpy()) \\\n",
        "    + [subword_zh.vocab_size+1]\n",
        "\n",
        "    return en_tmp, zh_tmp\n",
        "\n",
        "# tensorflow 為靜態計算圖，需透過eager mode的api讓python直譯器可以直接和tf運算核心互動\n",
        "def tf_add_BEOS(en_t, zh_t):\n",
        "    return tf.py_function(add_BEOS, [en_t, zh_t], [tf.int64, tf.int64])\n",
        "\n",
        "tmp_dataset = training_set.map(tf_add_BEOS)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ncr8lzLc5sI",
        "colab_type": "code",
        "outputId": "d80ae88d-e2ea-4c54-ff87-9f744eab1bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "# 訓練時每一組batch內的資料長度要一樣，要將batch裡的所有序列都用padded_batch到同樣長度\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        "en_batch, zh_batch = next(iter(tmp_dataset))\n",
        "print(\"英文索引序列的 batch\")\n",
        "print(en_batch)\n",
        "print('-' * 20)\n",
        "print(\"中文索引序列的 batch\")\n",
        "print(zh_batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "英文索引序列的 batch\n",
            "tf.Tensor(\n",
            "[[32007  4547  6859 ...     0     0     0]\n",
            " [32007  7943 31837 ...     0     0     0]\n",
            " [32007 24844 14087 ...     0     0     0]\n",
            " ...\n",
            " [32007 16445 31837 ...     0     0     0]\n",
            " [32007  4547  6112 ...     0     0     0]\n",
            " [32007 19682  2846 ...     0     0     0]], shape=(128, 161), dtype=int64)\n",
            "--------------------\n",
            "中文索引序列的 batch\n",
            "tf.Tensor(\n",
            "[[32061   146  2230 ...     0     0     0]\n",
            " [32061    44     1 ...     0     0     0]\n",
            " [32061   125  2545 ...     0     0     0]\n",
            " ...\n",
            " [32061    28     1 ...     0     0     0]\n",
            " [32061  1363 21783 ...     0     0     0]\n",
            " [32061   659  8696 ...     0     0     0]], shape=(128, 56), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS1CDwmWhodx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############  以上實驗  以下正式轉換  #########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3T02GdPeM6F",
        "colab_type": "code",
        "outputId": "5edae324-9be6-41b1-a58a-1cb501d31fa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# 正式將文字data一步轉換成訓練格式\n",
        "batch_size = 128\n",
        "shuffle_size = 15000\n",
        "\n",
        "training_dataset = (\n",
        "    training_set.map(tf_add_BEOS).cache().shuffle(shuffle_size)\n",
        "    .padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_dataset = (\n",
        "    val_set.map(tf_add_BEOS)\n",
        "    .padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        ")\n",
        "\n",
        "\n",
        "# 資料以一組一組的batch(size=128)儲存\n",
        "# check\n",
        "# en_batch, zh_batch = next(iter(training_dataset))\n",
        "# print(\"英文索引序列的 batch\")\n",
        "# print(en_batch)\n",
        "# print('-' * 20)\n",
        "# print(\"中文索引序列的 batch\")\n",
        "# print(zh_batch)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# 資料以一組一組的batch(size=128)儲存\\n# check\\nen_batch, zh_batch = next(iter(training_dataset))\\nprint(\"英文索引序列的 batch\")\\nprint(en_batch)\\nprint(\\'-\\' * 20)\\nprint(\"中文索引序列的 batch\")\\nprint(zh_batch)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd5fJfnog5Ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMu9Ih50g6He",
        "colab_type": "code",
        "outputId": "640e9c4f-33e1-4fed-df72-d2e90e8327f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# 測試資料\n",
        "demo_examples = [\n",
        "    (\"It is important.\", \"这很重要。\"),\n",
        "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
        "]\n",
        "pprint(demo_examples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('It is important.', '这很重要。'),\n",
            " ('The numbers speak for themselves.', '数字证明了一切。')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M11fLfpRhfUj",
        "colab_type": "code",
        "outputId": "fd7a9cad-aa10-4d48-8271-1be33c583cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "# 將測試資料建成tf dataset\n",
        "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
        "    [en for en, zh in demo_examples], [zh for en, zh in demo_examples]\n",
        "))\n",
        "\n",
        "try_data = demo_examples.map(tf_add_BEOS).padded_batch(2, padded_shapes=([-1], [-1]))\n",
        "\n",
        "en_try, zh_try = next(iter(try_data))\n",
        "print('en_try:', en_try)\n",
        "print('' * 10)\n",
        "print('zh_try:', zh_try)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "en_try: tf.Tensor(\n",
            "[[32007 31878  2132 12018  6514 23570 19665 31921 31851 32008     0     0\n",
            "      0     0     0     0]\n",
            " [32007  4547 24819 12017 11163 31837  9085 13718  7465  3051  6185  6358\n",
            "  27766 18206 31851 32008]], shape=(2, 16), dtype=int64)\n",
            "\n",
            "zh_try: tf.Tensor(\n",
            "[[32061 12332   346     2 32062     0]\n",
            " [32061   770  4435  1078     2 32062]], shape=(2, 6), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0nx_pkPlv8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word embedding\n",
        "en_dic_length = subword_en.vocab_size + 2 # 加上起始和結束的tokens\n",
        "zh_dic_length = subword_zh.vocab_size + 2\n",
        "\n",
        "# set embedding dimension\n",
        "dimen = 4\n",
        "embedding_layer_en = tf.keras.layers.Embedding(en_dic_length, dimen)\n",
        "embedding_layer_zh = tf.keras.layers.Embedding(zh_dic_length, dimen)\n",
        "\n",
        "emb_en_try = embedding_layer_en(en_try)\n",
        "emb_zh_try = embedding_layer_zh(zh_try)\n",
        "emb_en_try, emb_zh_try\n",
        "# 輸出中每個list(batch)有兩句話，每句話有16(en)/6(zh)個字，每個字用4個維度表示"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGa2MiqYukvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# padding mask\n",
        "# 因為短的序列有補0，不希望訓練時的attention放在0上，需要padding mask把序列是0的地方遮蔽\n",
        "# 概念類似讓序列是0的地方乘上很負的數，q和k點積完再經過softmax後的機率就會變得很低\n",
        "\n",
        "def create_padding_mask(sequence):\n",
        "    mask = tf.cast(tf.equal(sequence, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "    #　為了後續broadcasting，把張量維度補到4維\n",
        "\n",
        "en_try_mask = create_padding_mask(en_try)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0XOYfjQx4J-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################### self-attention ###############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvinoEEoyAHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set random seed\n",
        "tf.random.set_seed(9487)\n",
        "\n",
        "q, k = emb_en_try, emb_en_try # self-attention的q和k一樣\n",
        "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_en_try.shape), 0.5),\n",
        "    tf.float32) # 隨機產生跟q, k一樣shape的binary矩陣\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44-GeprNz42-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scaled dot product attention\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    # 將q, k做點積再scale\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
        "\n",
        "    # 先將logits加上mask\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9) \n",
        "        # mask乘上極大負值，讓序列中0的地方變極負，softmax完趨近於0\n",
        "\n",
        "    # 利用softmax計算總合為1的注意權重\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  \n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "    # 以注意權重對v做加權平均（weighted average）\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWzFFJle5T-J",
        "colab_type": "code",
        "outputId": "023b1b9d-f798-410b-dcd1-60e1bd6ab451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mask = en_try_mask\n",
        "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "print(\"output:\", output)\n",
        "print(\"-\" * 20)\n",
        "print(\"attention_weights:\", attention_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output: tf.Tensor(\n",
            "[[[[0.49994993 0.29993173 0.59999937 0.29995716]\n",
            "   [0.49998304 0.29983938 0.6000892  0.30020246]\n",
            "   [0.4999054  0.30020282 0.5996472  0.29974025]\n",
            "   [0.49995193 0.3002391  0.59978414 0.29985556]\n",
            "   [0.5000038  0.29993367 0.60008895 0.30000097]\n",
            "   [0.4999317  0.3000367  0.5998912  0.29985514]\n",
            "   [0.49994904 0.29974517 0.6000749  0.30022812]\n",
            "   [0.4999966  0.2999573  0.60012144 0.2998963 ]\n",
            "   [0.5000998  0.30003956 0.6001653  0.30007944]\n",
            "   [0.5000279  0.2998821  0.6001103  0.3001626 ]\n",
            "   [0.5000849  0.30025667 0.59992373 0.3001    ]\n",
            "   [0.5000849  0.30025667 0.59992373 0.3001    ]\n",
            "   [0.5000849  0.30025667 0.59992373 0.3001    ]\n",
            "   [0.5000849  0.30025667 0.59992373 0.3001    ]\n",
            "   [0.5000849  0.30025667 0.59992373 0.3001    ]\n",
            "   [0.5000849  0.30025667 0.59992373 0.3001    ]]\n",
            "\n",
            "  [[0.5000268  0.7000045  0.40006343 0.6999943 ]\n",
            "   [0.50002754 0.6999756  0.40006247 0.70000076]\n",
            "   [0.5000876  0.69979936 0.4001211  0.69987833]\n",
            "   [0.50004154 0.6998825  0.39989197 0.6998811 ]\n",
            "   [0.4999048  0.70015794 0.39998588 0.7001436 ]\n",
            "   [0.49991122 0.70012933 0.39982024 0.7000811 ]\n",
            "   [0.5000975  0.6998542  0.39993316 0.6998236 ]\n",
            "   [0.49993938 0.70011413 0.40000832 0.70008796]\n",
            "   [0.5000477  0.6998532  0.4001826  0.69995433]\n",
            "   [0.49992263 0.70013124 0.4000969  0.7001478 ]\n",
            "   [0.4999908  0.69994277 0.39993796 0.6999669 ]\n",
            "   [0.49990612 0.70024455 0.39992014 0.70017874]\n",
            "   [0.5000441  0.6999667  0.39983907 0.6998887 ]\n",
            "   [0.49994907 0.7002021  0.40007976 0.7001419 ]\n",
            "   [0.50007737 0.6997951  0.39998695 0.6998553 ]\n",
            "   [0.4999937  0.6999382  0.40000236 0.6999705 ]]]\n",
            "\n",
            "\n",
            " [[[0.4999687  0.43737048 0.5000627  0.4373864 ]\n",
            "   [0.49998942 0.4374004  0.5000552  0.43762732]\n",
            "   [0.49994087 0.43758476 0.49980998 0.43729562]\n",
            "   [0.49997002 0.43776187 0.49978337 0.4375223 ]\n",
            "   [0.5000024  0.43741453 0.5000876  0.4374566 ]\n",
            "   [0.4999573  0.43745166 0.4999838  0.4373381 ]\n",
            "   [0.4999681  0.43726116 0.5001046  0.43756306]\n",
            "   [0.4999979  0.43739724 0.50013125 0.43735912]\n",
            "   [0.50006235 0.43764383 0.5000166  0.43766874]\n",
            "   [0.5000174  0.43743563 0.50006217 0.43761092]\n",
            "   [0.5000529  0.43795997 0.49973434 0.4378622 ]\n",
            "   [0.5000529  0.43795997 0.49973434 0.4378622 ]\n",
            "   [0.5000529  0.43795997 0.49973434 0.4378622 ]\n",
            "   [0.5000529  0.43795997 0.49973434 0.4378622 ]\n",
            "   [0.5000529  0.43795997 0.49973434 0.4378622 ]\n",
            "   [0.5000529  0.43795997 0.49973434 0.4378622 ]]\n",
            "\n",
            "  [[0.43749768 0.56250334 0.56253076 0.56248593]\n",
            "   [0.4374739  0.5624999  0.5625726  0.5624889 ]\n",
            "   [0.4375744  0.5623275  0.5625535  0.56249666]\n",
            "   [0.43761194 0.5624132  0.56239104 0.5625238 ]\n",
            "   [0.43739027 0.5626166  0.562533   0.562497  ]\n",
            "   [0.4374611  0.5626168  0.56241155 0.5625215 ]\n",
            "   [0.43765965 0.5623913  0.56237745 0.5625123 ]\n",
            "   [0.43744397 0.5625674  0.56250256 0.56249475]\n",
            "   [0.43750447 0.5623573  0.5626209  0.5624883 ]\n",
            "   [0.43737158 0.5625814  0.5626051  0.562483  ]\n",
            "   [0.43753445 0.56244963 0.56245637 0.5625184 ]\n",
            "   [0.4373539  0.5627342  0.56253034 0.56249106]\n",
            "   [0.43762356 0.5624908  0.5623312  0.56251836]\n",
            "   [0.43738627 0.56265354 0.5625671  0.56246996]\n",
            "   [0.43760395 0.56234777 0.56247663 0.56251633]\n",
            "   [0.4375322  0.5624247  0.5624795  0.5625108 ]]]], shape=(2, 2, 16, 4), dtype=float32)\n",
            "--------------------\n",
            "attention_weights: tf.Tensor(\n",
            "[[[[0.10002542 0.09999493 0.09999904 ... 0.         0.\n",
            "    0.        ]\n",
            "   [0.09999541 0.10009833 0.09990221 ... 0.         0.\n",
            "    0.        ]\n",
            "   [0.10001721 0.09991986 0.10027296 ... 0.         0.\n",
            "    0.        ]\n",
            "   ...\n",
            "   [0.09992303 0.10002447 0.099957   ... 0.         0.\n",
            "    0.        ]\n",
            "   [0.09992303 0.10002447 0.099957   ... 0.         0.\n",
            "    0.        ]\n",
            "   [0.09992303 0.10002447 0.099957   ... 0.         0.\n",
            "    0.        ]]\n",
            "\n",
            "  [[0.1000349  0.10000364 0.10001455 ... 0.         0.\n",
            "    0.        ]\n",
            "   [0.10000878 0.10007828 0.10002215 ... 0.         0.\n",
            "    0.        ]\n",
            "   [0.10000993 0.10001238 0.10009831 ... 0.         0.\n",
            "    0.        ]\n",
            "   ...\n",
            "   [0.1000636  0.09998847 0.09994232 ... 0.         0.\n",
            "    0.        ]\n",
            "   [0.0999653  0.10003302 0.10007465 ... 0.         0.\n",
            "    0.        ]\n",
            "   [0.09998301 0.09994595 0.10001872 ... 0.         0.\n",
            "    0.        ]]]\n",
            "\n",
            "\n",
            " [[[0.06253958 0.06252051 0.06252308 ... 0.06246053 0.06246053\n",
            "    0.06246053]\n",
            "   [0.06249692 0.06256124 0.06243866 ... 0.06250035 0.06250035\n",
            "    0.06250035]\n",
            "   [0.06252222 0.06246137 0.06268208 ... 0.0624809  0.0624809\n",
            "    0.0624809 ]\n",
            "   ...\n",
            "   [0.0623702  0.06243352 0.0623914  ... 0.06263626 0.06263626\n",
            "    0.06263626]\n",
            "   [0.0623702  0.06243352 0.0623914  ... 0.06263626 0.06263626\n",
            "    0.06263626]\n",
            "   [0.0623702  0.06243352 0.0623914  ... 0.06263626 0.06263626\n",
            "    0.06263626]]\n",
            "\n",
            "  [[0.06252478 0.06250524 0.06251206 ... 0.06254394 0.06247765\n",
            "    0.06249401]\n",
            "   [0.06250636 0.06254979 0.06251471 ... 0.0624981  0.06252109\n",
            "    0.06247196]\n",
            "   [0.06251144 0.06251297 0.06256669 ... 0.06246751 0.06254537\n",
            "    0.06251571]\n",
            "   ...\n",
            "   [0.06254456 0.0624976  0.06246876 ... 0.06264264 0.06239373\n",
            "    0.0624819 ]\n",
            "   [0.06247614 0.06251847 0.06254448 ... 0.06239161 0.06259373\n",
            "    0.06250707]\n",
            "   [0.06249103 0.06246786 0.06251334 ... 0.06247829 0.0625056\n",
            "    0.06253195]]]], shape=(2, 2, 16, 16), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}